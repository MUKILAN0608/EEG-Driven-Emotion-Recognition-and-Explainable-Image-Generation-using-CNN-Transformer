{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgK94boxaX96"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TUguQ3-rahU6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "\n",
        "# Download latest version of the GAMEEMO dataset\n",
        "path = kagglehub.dataset_download(\"sigfest/database-for-emotion-recognition-system-gameemo\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n"
      ],
      "metadata": {
        "id": "bTMF4aeYpcIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "base_path = \"/root/.cache/kagglehub/datasets/sigfest/database-for-emotion-recognition-system-gameemo/versions/1\"\n",
        "dataset_path = os.path.join(base_path, \"GAMEEMO\")\n",
        "print(dataset_path)\n"
      ],
      "metadata": {
        "id": "Sd-XN9sYqk6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FBsiYatUqkEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "for item in os.listdir(dataset_path):\n",
        "    print(item)\n"
      ],
      "metadata": {
        "id": "aiRpwTQIpcZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_path = \"/root/.cache/kagglehub/datasets/sigfest/database-for-emotion-recognition-system-gameemo/versions/1/GAMEEMO\"\n",
        "subjects = sorted([f for f in os.listdir(base_path) if f.startswith(\"(S\")])\n",
        "subjects\n"
      ],
      "metadata": {
        "id": "AjF7caCdq1kZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_raw_eeg(subject_folder):\n",
        "    raw_path = os.path.join(subject_folder, \"Raw EEG Data\")\n",
        "    for f in os.listdir(raw_path):\n",
        "        if f.lower().endswith(\".csv\"):\n",
        "            return pd.read_csv(os.path.join(raw_path, f))\n",
        "    return None\n",
        "\n",
        "def load_sam(subject_folder):\n",
        "    sam_path = os.path.join(subject_folder, \"SAM Ratings\")\n",
        "    for f in os.listdir(sam_path):\n",
        "        if f.lower().endswith(\".csv\"):\n",
        "            return pd.read_csv(os.path.join(sam_path, f))\n",
        "    return None\n"
      ],
      "metadata": {
        "id": "o4lG3HxIqMjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "dataset_path = \"/root/.cache/kagglehub/datasets/sigfest/database-for-emotion-recognition-system-gameemo/versions/1/GAMEEMO\"\n",
        "\n",
        "for root, dirs, files in os.walk(dataset_path):\n",
        "    print(root)\n",
        "    for f in files:\n",
        "        print(\"   FILE:\", f)\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "dJLOq1QPrXvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "base_path = \"/root/.cache/kagglehub/datasets/sigfest/database-for-emotion-recognition-system-gameemo/versions/1/GAMEEMO\"\n",
        "subjects = sorted([f for f in os.listdir(base_path) if f.startswith(\"(S\")])\n",
        "\n",
        "emotion_map = {\n",
        "    \"G1\": \"Calm\",\n",
        "    \"G2\": \"Funny\",\n",
        "    \"G3\": \"Horror\",\n",
        "    \"G4\": \"Boring\"\n",
        "}\n",
        "\n",
        "all_data = []\n",
        "\n",
        "for subject in subjects:\n",
        "    subject_path = os.path.join(base_path, subject)\n",
        "    raw_csv_path = os.path.join(subject_path, \"Raw EEG Data/.csv format\")\n",
        "\n",
        "    if not os.path.exists(raw_csv_path):\n",
        "        print(\"No RAW CSV for\", subject)\n",
        "        continue\n",
        "\n",
        "    for file in os.listdir(raw_csv_path):\n",
        "        if file.endswith(\".csv\"):\n",
        "            file_path = os.path.join(raw_csv_path, file)\n",
        "\n",
        "            # Find G1/G2/G3/G4 from filename\n",
        "            # Example: S11G3AllRawChannels.csv → G3\n",
        "            gcode = file.split(\"G\")[1][0]   # extracts \"3\" from \"G3\"\n",
        "            gkey = f\"G{gcode}\"\n",
        "            emotion = emotion_map[gkey]\n",
        "\n",
        "            df = pd.read_csv(file_path)\n",
        "            df[\"subject\"] = subject\n",
        "            df[\"gameplay\"] = gkey\n",
        "            df[\"emotion\"] = emotion\n",
        "\n",
        "            all_data.append(df)\n",
        "\n",
        "    print(\"Processed:\", subject)\n",
        "\n",
        "# Combine all subjects\n",
        "gameemo_raw_df = pd.concat(all_data, ignore_index=True)\n",
        "gameemo_raw_df.shape\n"
      ],
      "metadata": {
        "id": "KC3xCkvYr661"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gameemo_raw_df.head()\n"
      ],
      "metadata": {
        "id": "8CsFg_a2sJmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean dataset from unwanted columns\n",
        "cols_to_drop = [col for col in gameemo_raw_df.columns if \"Unnamed\" in col or col == \"eeg\"]\n",
        "\n",
        "gameemo_raw_df = gameemo_raw_df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "print(\"Cleaned columns:\")\n",
        "print(gameemo_raw_df.columns)\n",
        "gameemo_raw_df.head()\n"
      ],
      "metadata": {
        "id": "vXpnHutKwess"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert categorical labels to numeric\n",
        "gameemo_raw_df['emotion_id'] = gameemo_raw_df['emotion'].astype('category').cat.codes\n",
        "gameemo_raw_df['subject_id'] = gameemo_raw_df['subject'].astype('category').cat.codes\n",
        "\n",
        "print(gameemo_raw_df[['emotion','emotion_id']].drop_duplicates())\n"
      ],
      "metadata": {
        "id": "_UzQPRi_GwPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_channels = ['FC5','FC6','O1','O2','P7','P8','T7','T8']\n",
        "\n",
        "X = gameemo_raw_df[eeg_channels].astype('float32').values\n",
        "y = gameemo_raw_df['emotion_id'].values\n",
        "\n",
        "print(\"EEG shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n"
      ],
      "metadata": {
        "id": "s5YAwlN1HAZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Z-score normalization\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0) + 1e-8\n",
        "X_norm = (X - X_mean) / X_std\n",
        "\n",
        "print(\"Normalized EEG sample:\", X_norm[0])\n"
      ],
      "metadata": {
        "id": "bKJC2DxCHERL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "WINDOW = 256  # number of time samples per window\n",
        "STEP = 256     # no overlap\n",
        "\n",
        "X_windows = []\n",
        "y_windows = []\n",
        "\n",
        "num_samples = len(X_norm)\n",
        "\n",
        "for start in range(0, num_samples - WINDOW, STEP):\n",
        "    end = start + WINDOW\n",
        "    X_windows.append(X_norm[start:end])\n",
        "    y_windows.append(y[start])\n",
        "\n",
        "X_windows = np.array(X_windows)\n",
        "y_windows = np.array(y_windows)\n",
        "\n",
        "print(\"Windowed EEG shape:\", X_windows.shape)\n",
        "print(\"Windowed labels shape:\", y_windows.shape)\n"
      ],
      "metadata": {
        "id": "GTv8adhXHIBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_windows, y_windows, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ],
      "metadata": {
        "id": "sAJWnidrHO8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx].permute(1,0), self.y[idx]\n",
        "        # permute → (channels, time)\n",
        "\n",
        "train_ds = EEGDataset(X_train, y_train)\n",
        "test_ds = EEGDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=64)\n"
      ],
      "metadata": {
        "id": "duzsJsMtHTwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EEGTransformer(nn.Module):\n",
        "    def __init__(self, num_channels=8, num_classes=4, embed_dim=64, num_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_proj = nn.Linear(num_channels, embed_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, channels, time)\n",
        "        x = x.permute(0,2,1)  # → (batch, time, channels)\n",
        "        x = self.input_proj(x)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)  # pooled\n",
        "        return self.classifier(x)\n",
        "\n",
        "model = EEGTransformer(num_channels=8, num_classes=len(set(y_windows)))\n"
      ],
      "metadata": {
        "id": "EIg_NYnLHcVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(Xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {total_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "HnkY0QvjHfi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# =======================\n",
        "# STRONG MODEL COMPONENTS\n",
        "# =======================\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, t = x.size()\n",
        "        y = self.pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class EEG_Strong_Model(nn.Module):\n",
        "    def __init__(self, num_channels=8, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv1d(num_channels, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.cnn2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.cnn3 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.attn = SEBlock(256)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=256, nhead=8, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.cnn1(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.bn2(self.cnn2(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.bn3(self.cnn3(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.attn(x)\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# =======================\n",
        "# MODEL INITIALIZATION\n",
        "# =======================\n",
        "\n",
        "# Calculate class weights\n",
        "class_counts = np.bincount(y_train)\n",
        "class_weights = torch.tensor(1. / class_counts, dtype=torch.float32)\n",
        "class_weights /= class_weights.sum()\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "model = EEG_Strong_Model(\n",
        "    num_channels=8,\n",
        "    num_classes=len(np.unique(y_train))\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights,\n",
        "    label_smoothing=0.1\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=40,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# EARLY STOP TRAINING\n",
        "# =======================\n",
        "\n",
        "EPOCHS = 40\n",
        "PATIENCE = 7\n",
        "best_val_loss = np.inf\n",
        "patience_counter = 0\n",
        "best_model_path = \"best_gameemo_strong_model.pth\"\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        # EEG noise augmentation\n",
        "        Xb = Xb + 0.01 * torch.randn_like(Xb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(Xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in test_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "            preds = model(Xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pred_labels = torch.argmax(preds, dim=1)\n",
        "            all_preds.extend(pred_labels.cpu().numpy())\n",
        "            all_true.extend(yb.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_acc = accuracy_score(all_true, all_preds)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Early stopping logic\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(\"Best model updated\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement. Patience {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# =======================\n",
        "# LOAD BEST MODEL\n",
        "# =======================\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "print(\"Best model loaded\")\n",
        "\n",
        "# =======================\n",
        "# FINAL EVALUATION\n",
        "# =======================\n",
        "\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "        preds = model(Xb)\n",
        "        pred_labels = torch.argmax(preds, dim=1)\n",
        "\n",
        "        all_preds.extend(pred_labels.cpu().numpy())\n",
        "        all_true.extend(yb.cpu().numpy())\n",
        "\n",
        "final_acc = accuracy_score(all_true, all_preds)\n",
        "\n",
        "print(\"Final best-stop accuracy:\", final_acc)\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(all_true, all_preds))"
      ],
      "metadata": {
        "id": "5z0LfyqdIrQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ============================================================\n",
        "# 1. CLEAN DATASET\n",
        "# ============================================================\n",
        "\n",
        "cols_to_drop = [col for col in gameemo_raw_df.columns if \"Unnamed\" in col or col == \"eeg\"]\n",
        "gameemo_raw_df = gameemo_raw_df.drop(columns=cols_to_drop, errors='ignore')\n",
        "\n",
        "gameemo_raw_df['emotion_id'] = gameemo_raw_df['emotion'].astype('category').cat.codes\n",
        "\n",
        "eeg_channels = ['FC5','FC6','O1','O2','P7','P8','T7','T8']\n",
        "\n",
        "X = gameemo_raw_df[eeg_channels].astype('float32').values\n",
        "y = gameemo_raw_df['emotion_id'].values\n",
        "\n",
        "print(\"Raw EEG shape:\", X.shape)\n",
        "print(\"Labels shape:\", y.shape)\n",
        "\n",
        "# ============================================================\n",
        "# 2. NORMALIZATION\n",
        "# ============================================================\n",
        "\n",
        "X_mean = X.mean(axis=0)\n",
        "X_std = X.std(axis=0) + 1e-8\n",
        "X_norm = (X - X_mean) / X_std\n",
        "\n",
        "# ============================================================\n",
        "# 3. OVERLAPPED WINDOWING (LARGER WINDOW)\n",
        "# ============================================================\n",
        "\n",
        "WINDOW = 512      # more context than before (256)\n",
        "STEP = 256        # 50% overlap\n",
        "\n",
        "X_windows, y_windows = [], []\n",
        "\n",
        "for start in range(0, len(X_norm) - WINDOW, STEP):\n",
        "    end = start + WINDOW\n",
        "    X_windows.append(X_norm[start:end])\n",
        "    y_windows.append(y[start])\n",
        "\n",
        "X_windows = np.array(X_windows)\n",
        "y_windows = np.array(y_windows)\n",
        "\n",
        "print(\"Windowed EEG shape:\", X_windows.shape)\n",
        "print(\"Windowed labels shape:\", y_windows.shape)\n",
        "\n",
        "# ============================================================\n",
        "# 4. TRAIN / TEST SPLIT\n",
        "# ============================================================\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_windows, y_windows,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_windows\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 5. CLASS WEIGHTS\n",
        "# ============================================================\n",
        "\n",
        "class_counts = Counter(y_train)\n",
        "total = sum(class_counts.values())\n",
        "class_weights = [total / class_counts[i] for i in sorted(class_counts)]\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "# ============================================================\n",
        "# 6. PYTORCH DATASET\n",
        "# ============================================================\n",
        "\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Return (channels, time)\n",
        "        return self.X[idx].permute(1,0), self.y[idx]\n",
        "\n",
        "train_ds = EEGDataset(X_train, y_train)\n",
        "test_ds = EEGDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=128)\n",
        "\n",
        "# ============================================================\n",
        "# 7. STRONG MODEL WITH ATTENTION (TUNED)\n",
        "# ============================================================\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channels, channels // reduction),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(channels // reduction, channels),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, t = x.size()\n",
        "        y = self.pool(x).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y\n",
        "\n",
        "\n",
        "class EEG_Strong_Model(nn.Module):\n",
        "    def __init__(self, num_channels=8, num_classes=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv1d(num_channels, 64, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "\n",
        "        self.cnn2 = nn.Conv1d(64, 128, kernel_size=5, padding=2)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "\n",
        "        self.cnn3 = nn.Conv1d(128, 256, kernel_size=5, padding=2)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "\n",
        "        self.attn = SEBlock(256)\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=256, nhead=8, batch_first=True\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),   # slightly less dropout than before\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        x = self.relu(self.bn1(self.cnn1(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.bn2(self.cnn2(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.relu(self.bn3(self.cnn3(x)))\n",
        "        x = self.pool(x)\n",
        "\n",
        "        x = self.attn(x)\n",
        "        x = x.permute(0,2,1)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        return self.classifier(x)\n",
        "\n",
        "# ============================================================\n",
        "# 8. MODEL, LOSS, OPTIMIZER, SCHEDULER\n",
        "# ============================================================\n",
        "\n",
        "model = EEG_Strong_Model(\n",
        "    num_channels=8,\n",
        "    num_classes=len(np.unique(y_train))\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(\n",
        "    weight=class_weights\n",
        ")  # no label smoothing this time\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-2)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "    optimizer,\n",
        "    T_max=50,\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# 9. EARLY STOP TRAINING (TUNED)\n",
        "# ============================================================\n",
        "\n",
        "EPOCHS = 50\n",
        "PATIENCE = 10\n",
        "best_val_loss = np.inf\n",
        "patience_counter = 0\n",
        "best_model_path = \"best_gameemo_model_tuned.pth\"\n",
        "\n",
        "history = {\n",
        "    'train_loss': [],\n",
        "    'train_acc': [],\n",
        "    'val_loss': [],\n",
        "    'val_acc': []\n",
        "}\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_preds = []\n",
        "    train_true = []\n",
        "\n",
        "    for Xb, yb in train_loader:\n",
        "        Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "        # Slight noise augmentation\n",
        "        Xb = Xb + 0.005 * torch.randn_like(Xb)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(Xb)\n",
        "        loss = criterion(preds, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        pred_labels = torch.argmax(preds, dim=1)\n",
        "        train_preds.extend(pred_labels.cpu().numpy())\n",
        "        train_true.extend(yb.cpu().numpy())\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc = accuracy_score(train_true, train_preds)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    all_preds = []\n",
        "    all_true = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for Xb, yb in test_loader:\n",
        "            Xb, yb = Xb.to(device), yb.to(device)\n",
        "\n",
        "            preds = model(Xb)\n",
        "            loss = criterion(preds, yb)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            pred_labels = torch.argmax(preds, dim=1)\n",
        "            all_preds.extend(pred_labels.cpu().numpy())\n",
        "            all_true.extend(yb.cpu().numpy())\n",
        "\n",
        "    val_loss /= len(test_loader)\n",
        "    val_acc = accuracy_score(all_true, all_preds)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Val Acc: {val_acc:.4f}\"\n",
        "    )\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(\"Best model saved\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        print(f\"No improvement. Patience {patience_counter}/{PATIENCE}\")\n",
        "\n",
        "        if patience_counter >= PATIENCE:\n",
        "            print(\"Early stopping triggered\")\n",
        "            break\n",
        "\n",
        "# ============================================================\n",
        "# 10. LOAD BEST MODEL AND FINAL EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path))\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_true = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "        yb = yb.to(device)\n",
        "        preds = model(Xb)\n",
        "        pred_labels = torch.argmax(preds, dim=1)\n",
        "\n",
        "        all_preds.extend(pred_labels.cpu().numpy())\n",
        "        all_true.extend(yb.cpu().numpy())\n",
        "\n",
        "final_acc = accuracy_score(all_true, all_preds)\n",
        "\n",
        "print(\"Final tuned best-stop accuracy:\", final_acc)\n",
        "print(\"Classification report:\")\n",
        "print(classification_report(all_true, all_preds))\n"
      ],
      "metadata": {
        "id": "whIFxC4oYGR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Jb8pwZUtWBU2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# POST-TRAINING VISUALIZATION FOR GAMEEMO EEG\n",
        "# ============================================================\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ============================================\n",
        "# 1. LOAD YOUR BEST MODEL\n",
        "# ============================================\n",
        "\n",
        "model_path = \"best_gameemo_model_tuned.pth\"   # change if needed\n",
        "\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded model:\", model_path)\n",
        "\n",
        "# ============================================\n",
        "# 2. GET PREDICTIONS ON TEST SET\n",
        "# ============================================\n",
        "\n",
        "all_preds = []\n",
        "all_true = []\n",
        "all_embeddings = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "\n",
        "        # Predictions\n",
        "        preds = model(Xb)\n",
        "        pred_labels = torch.argmax(preds, dim=1)\n",
        "\n",
        "        all_preds.extend(pred_labels.cpu().numpy())\n",
        "        all_true.extend(yb.numpy())\n",
        "\n",
        "        # Embeddings (for PCA)\n",
        "        emb = model.forward_features(Xb)\n",
        "        all_embeddings.append(emb.cpu().numpy())\n",
        "\n",
        "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "\n",
        "# ============================================\n",
        "# 3. CONFUSION MATRIX VISUALIZATION\n",
        "# ============================================\n",
        "\n",
        "cm = confusion_matrix(all_true, all_preds)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix - EEG Emotion Model\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 4. CLASSIFICATION REPORT\n",
        "# ============================================\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(all_true, all_preds))\n",
        "\n",
        "# ============================================\n",
        "# 5. PREDICTION DISTRIBUTION BAR PLOT\n",
        "# ============================================\n",
        "\n",
        "plt.figure(figsize=(5, 4))\n",
        "plt.hist(all_preds, bins=4)\n",
        "plt.title(\"Prediction Distribution\")\n",
        "plt.xlabel(\"Emotion Class\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# ============================================\n",
        "# 6. EEG EMBEDDING VISUALIZATION (PCA)\n",
        "# ============================================\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "emb_2d = pca.fit_transform(all_embeddings)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.scatter(emb_2d[:, 0], emb_2d[:, 1], c=all_true, s=5)\n",
        "plt.title(\"2D Visualization of EEG Emotion Embeddings (PCA)\")\n",
        "plt.xlabel(\"PCA Component 1\")\n",
        "plt.ylabel(\"PCA Component 2\")\n",
        "plt.colorbar(label=\"Emotion Class\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YCRahb4VcVE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "# Assuming you have:\n",
        "# all_embeddings - your EEG embeddings\n",
        "# all_true - your emotion labels (0, 1, 2, 3)\n",
        "\n",
        "# Define emotion mapping\n",
        "emotion_mapping = {\n",
        "    0: 'Boring',\n",
        "    1: 'Calm',\n",
        "    2: 'Funny',\n",
        "    3: 'Horror'\n",
        "}\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "emb_2d = pca.fit_transform(all_embeddings)\n",
        "\n",
        "# Create the visualization with proper labels\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Create scatter plot\n",
        "scatter = plt.scatter(emb_2d[:, 0], emb_2d[:, 1],\n",
        "                     c=all_true,\n",
        "                     s=20,  # Increased size for better visibility\n",
        "                     cmap='viridis',\n",
        "                     alpha=0.6)\n",
        "\n",
        "# Set labels and title\n",
        "plt.title(\"2D Visualization of EEG Emotion Embeddings (PCA)\",\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.xlabel(\"PCA Component 1\", fontsize=12)\n",
        "plt.ylabel(\"PCA Component 2\", fontsize=12)\n",
        "\n",
        "# Add colorbar with emotion labels\n",
        "cbar = plt.colorbar(scatter, label=\"Emotion Class\")\n",
        "cbar.set_label('Emotion Class', rotation=270, labelpad=20, fontsize=12)\n",
        "\n",
        "# Set colorbar ticks to match emotion classes\n",
        "cbar.set_ticks([0, 1, 2, 3])\n",
        "cbar.set_ticklabels(['Boring (0)', 'Calm (1)', 'Funny (2)', 'Horror (3)'])\n",
        "\n",
        "# Optional: Add legend with emotion names\n",
        "from matplotlib.patches import Patch\n",
        "legend_elements = [\n",
        "    Patch(facecolor=plt.cm.viridis(0.0), label='Boring (0)', alpha=0.6),\n",
        "    Patch(facecolor=plt.cm.viridis(0.33), label='Calm (1)', alpha=0.6),\n",
        "    Patch(facecolor=plt.cm.viridis(0.66), label='Funny (2)', alpha=0.6),\n",
        "    Patch(facecolor=plt.cm.viridis(1.0), label='Horror (3)', alpha=0.6)\n",
        "]\n",
        "plt.legend(handles=legend_elements, loc='best', fontsize=10, framealpha=0.9)\n",
        "\n",
        "# Add grid for better readability\n",
        "plt.grid(True, alpha=0.3, linestyle='--')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Print statistics\n",
        "print(\"\\nEmotion Distribution:\")\n",
        "unique, counts = np.unique(all_true, return_counts=True)\n",
        "for emotion_id, count in zip(unique, counts):\n",
        "    emotion_name = emotion_mapping.get(emotion_id, f'Unknown')\n",
        "    print(f\"{emotion_name} ({emotion_id}): {count} samples ({count/len(all_true)*100:.1f}%)\")\n",
        "\n",
        "print(f\"\\nExplained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_)*100:.2f}%\")"
      ],
      "metadata": {
        "id": "d9ECz-6xdWqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers transformers accelerate safetensors\n"
      ],
      "metadata": {
        "id": "KGLSCw5jnNM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_path = \"best_gameemo_strong_model.pth\"\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"Loaded model:\", model_path)\n"
      ],
      "metadata": {
        "id": "K1AUbF-u7BRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_embeddings = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for Xb, yb in test_loader:\n",
        "        Xb = Xb.to(device)\n",
        "\n",
        "        features = model.forward_features(Xb)   # (batch, 256)\n",
        "\n",
        "        all_embeddings.append(features.cpu().numpy())\n",
        "        all_labels.append(yb.numpy())\n",
        "\n",
        "all_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "np.save(\"eeg_emotion_embeddings.npy\", all_embeddings)\n",
        "np.save(\"eeg_emotion_labels.npy\", all_labels)\n",
        "\n",
        "print(\"Saved EEG embeddings:\", all_embeddings.shape)\n"
      ],
      "metadata": {
        "id": "EcknRp2RAmaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q diffusers transformers accelerate safetensors\n"
      ],
      "metadata": {
        "id": "XrhKk5lkArU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "class EEGtoCLIPAdapter(nn.Module):\n",
        "    def __init__(self, eeg_dim=256, clip_dim=768):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(eeg_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, clip_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "adapter = EEGtoCLIPAdapter().to(device)\n",
        "\n",
        "# Load the pipeline with torch.float32 to ensure consistent dtypes for training\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float32, # Changed to float32\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "\n",
        "tokenizer = pipe.tokenizer\n",
        "text_encoder = pipe.text_encoder"
      ],
      "metadata": {
        "id": "pWS7H4NxAsxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_prompts = {\n",
        "    0: \"boring dull low energy environment\",\n",
        "    1: \"happy joyful colorful energetic environment\",\n",
        "    2: \"horror dark frightening tense environment\",\n",
        "    3: \"calm peaceful relaxed soft lighting environment\"\n",
        "}\n",
        "\n",
        "def get_text_embedding(prompt):\n",
        "    tokens = tokenizer(\n",
        "        prompt,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=77,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        emb = text_encoder(**tokens).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    return emb\n",
        "\n",
        "text_targets = {k: get_text_embedding(v) for k, v in emotion_prompts.items()}\n"
      ],
      "metadata": {
        "id": "i0rJp5BKCMec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eeg_embeddings = np.load(\"eeg_emotion_embeddings.npy\")\n",
        "eeg_labels = np.load(\"eeg_emotion_labels.npy\")\n",
        "\n",
        "# Create a Dataset for EEG embeddings and labels\n",
        "class CLIPDataset(Dataset):\n",
        "    def __init__(self, embeddings, labels):\n",
        "        self.embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.embeddings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "clip_ds = CLIPDataset(eeg_embeddings, eeg_labels)\n",
        "clip_loader = DataLoader(clip_ds, batch_size=64, shuffle=True) # Use a reasonable batch size\n",
        "\n",
        "optimizer = torch.optim.Adam(adapter.parameters(), lr=3e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "for epoch in range(30):\n",
        "    total_loss = 0\n",
        "    for eeg_batch, label_batch in clip_loader:\n",
        "        eeg_batch = eeg_batch.to(device)\n",
        "        label_batch = label_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        pred_clip = adapter(eeg_batch)\n",
        "\n",
        "        target_clip = torch.stack([\n",
        "            text_targets[int(lbl.item())] for lbl in label_batch\n",
        "        ]).squeeze(1) # Squeeze to remove the extra dimension\n",
        "\n",
        "        loss = criterion(pred_clip, target_clip)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Loss: {total_loss / len(clip_loader):.6f}\")\n",
        "\n",
        "torch.save(adapter.state_dict(), \"eeg_to_clip_adapter.pth\")\n",
        "print(\"Adapter saved.\")"
      ],
      "metadata": {
        "id": "YCUdnr61CsT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_image_from_eeg(eeg_vector):\n",
        "    adapter.eval()\n",
        "\n",
        "    eeg_vector = torch.tensor(eeg_vector, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        clip_cond = adapter(eeg_vector)\n",
        "        # Stable Diffusion's text encoder typically outputs embeddings of shape (batch, 77, embed_dim).\n",
        "        # We need to expand our single embedding (batch, embed_dim) to match this structure.\n",
        "        clip_cond = clip_cond.unsqueeze(1).repeat(1, 77, 1) # Reshape to (batch, 77, embed_dim)\n",
        "\n",
        "    image = pipe(\n",
        "        prompt_embeds=clip_cond,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=7.5\n",
        "    ).images[0]\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "2NqUC-CgF9BM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "for i in range(5):\n",
        "    img = generate_image_from_eeg(eeg_embeddings[i])\n",
        "    img.save(f\"eeg_generated_{i}.png\")\n",
        "    print(f\"Saved eeg_generated_{i}.png\")\n"
      ],
      "metadata": {
        "id": "tLYfD01yGA3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "image_files = [f for f in os.listdir(\"/content\") if f.startswith(\"eeg_generated\")]\n",
        "\n",
        "plt.figure(figsize=(15, 5))\n",
        "for i, fname in enumerate(sorted(image_files)):\n",
        "    img = Image.open(fname)\n",
        "    plt.subplot(1, len(image_files), i + 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(fname)\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BWOyEJWtIEIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "\n",
        "print(\"Stable Diffusion loaded on:\", device)\n"
      ],
      "metadata": {
        "id": "6f84oUh2KiIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_prompts = {\n",
        "    0: \"a dull, empty, low-light room with faded colors, very boring and lifeless scene\",\n",
        "    1: \"a bright, colorful, joyful scene with smiling people and sunshine, very happy mood\",\n",
        "    2: \"a dark, scary, horror environment at night with fog, shadows and fear\",\n",
        "    3: \"a calm, peaceful lakeside or nature scene at sunset, relaxing and quiet\"\n",
        "}\n"
      ],
      "metadata": {
        "id": "T_5QZSNQLJbH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "eeg_labels = np.load(\"eeg_emotion_labels.npy\")   # shape (N,)\n",
        "print(eeg_labels[:10])\n"
      ],
      "metadata": {
        "id": "Wa9vyCw5LL7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "num_samples = 5   # how many EEG samples you want to visualize\n",
        "\n",
        "for i in range(num_samples):\n",
        "    emo_id = int(eeg_labels[i])\n",
        "    prompt = emotion_prompts[emo_id]\n",
        "    print(f\"EEG sample {i} → emotion {emo_id} → prompt: {prompt}\")\n",
        "\n",
        "    image = pipe(\n",
        "        prompt,\n",
        "        num_inference_steps=30,\n",
        "        guidance_scale=7.5\n",
        "    ).images[0]\n",
        "\n",
        "    image.save(f\"eeg_scene_{i}_class_{emo_id}.png\")\n",
        "    print(f\"Saved: eeg_scene_{i}_class_{emo_id}.png\")\n"
      ],
      "metadata": {
        "id": "_OxKUUU2LP0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# GRAD-CAM IMPLEMENTATION FOR EEG (1D CNN)\n",
        "# ============================================\n",
        "\n",
        "class EEGGradCAM:\n",
        "    def __init__(self, model, target_layer):\n",
        "        self.model = model\n",
        "        self.target_layer = target_layer\n",
        "\n",
        "        self.activations = None\n",
        "        self.gradients = None\n",
        "\n",
        "        self._register_hooks()\n",
        "\n",
        "    def _register_hooks(self):\n",
        "        def forward_hook(module, input, output):\n",
        "            self.activations = output.detach()\n",
        "\n",
        "        def backward_hook(module, grad_in, grad_out):\n",
        "            self.gradients = grad_out[0].detach()\n",
        "\n",
        "        self.target_layer.register_forward_hook(forward_hook)\n",
        "        self.target_layer.register_backward_hook(backward_hook)\n",
        "\n",
        "    def generate_cam(self, x, class_idx):\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        output = self.model(x)\n",
        "        target = output[:, class_idx]\n",
        "        target.backward()\n",
        "\n",
        "        weights = self.gradients.mean(dim=2, keepdim=True)\n",
        "        cam = (weights * self.activations).sum(dim=1)\n",
        "        cam = torch.relu(cam)\n",
        "\n",
        "        cam = cam - cam.min()\n",
        "        cam = cam / (cam.max() + 1e-8)\n",
        "\n",
        "        return cam.squeeze().cpu().numpy()\n"
      ],
      "metadata": {
        "id": "he_7xa2TM599"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradcam = EEGGradCAM(\n",
        "    model=model,\n",
        "    target_layer=model.cnn3\n",
        ")\n"
      ],
      "metadata": {
        "id": "K99kk9o0PFuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "# Take one EEG test sample\n",
        "X_sample, y_sample = next(iter(test_loader))\n",
        "X_sample = X_sample[0:1].to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pred = torch.argmax(model(X_sample)).item()\n",
        "\n",
        "cam = gradcam.generate_cam(X_sample, pred)\n",
        "\n",
        "print(\"Predicted Emotion Class:\", pred)\n",
        "print(\"CAM Shape:\", cam.shape)\n"
      ],
      "metadata": {
        "id": "l-uvTyNWPJyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "# Reshape cam to a 2D array (1 row, 128 columns) for imshow\n",
        "plt.imshow(cam.reshape(1, -1), aspect='auto', cmap='hot', origin='lower')\n",
        "plt.colorbar(label=\"Importance\")\n",
        "plt.title(\"EEG Grad-CAM Explainability Map\")\n",
        "plt.xlabel(\"Time\")\n",
        "plt.ylabel(\"Channel Importance (Aggregated)\") # Changed label to reflect 1D visualization\n",
        "plt.yticks([]) # Hide y-axis ticks as it's a single row\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PUlcFGt3PQrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate saliency map for input channel importance\n",
        "model.eval()\n",
        "X_sample.requires_grad_(True) # Enable gradient tracking for the input\n",
        "\n",
        "pred_output = model(X_sample)\n",
        "pred_label = torch.argmax(pred_output, dim=1)\n",
        "\n",
        "# Backpropagate a scalar for the predicted class to get gradients w.r.t. input\n",
        "one_hot = torch.zeros_like(pred_output).scatter_(1, pred_label.unsqueeze(-1), 1)\n",
        "pred_output.backward(gradient=one_hot, retain_graph=False)\n",
        "\n",
        "# Get gradients w.r.t. input and take absolute values\n",
        "gradients = X_sample.grad.abs()\n",
        "\n",
        "# Average gradients over the time dimension for each channel\n",
        "channel_importance = gradients.mean(dim=2).squeeze().cpu().numpy() # shape (8,)\n",
        "\n",
        "eeg_channels = ['FC5','FC6','O1','O2','P7','P8','T7','T8']\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(eeg_channels, channel_importance)\n",
        "plt.title(\"EEG Channel Importance for Emotion Prediction (Saliency Map)\")\n",
        "plt.ylabel(\"Importance Score\")\n",
        "plt.xlabel(\"EEG Channel\")\n",
        "plt.show()\n",
        "\n",
        "# Reset gradients and requires_grad_ for subsequent runs\n",
        "X_sample.grad.zero_()\n",
        "X_sample.requires_grad_(False)"
      ],
      "metadata": {
        "id": "JA33f5SnPhOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FULL EEG → IMAGE → XAI → CLEAR BOTTOM PANEL CODE (FINAL)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import ImageDraw, ImageFont\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EEG CHANNEL NAMES\n",
        "# ------------------------------------------------------------\n",
        "eeg_channels = ['FC5','FC6','O1','O2','P7','P8','T7','T8']\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EMOTION → PSYCHOLOGICAL FACTORS\n",
        "# ------------------------------------------------------------\n",
        "emotion_factors = {\n",
        "    0: {\"name\": \"BORING\", \"valence\": \"Low\",  \"arousal\": \"Low\",  \"lighting\": \"Dull\",   \"motion\": \"Static\"},\n",
        "    1: {\"name\": \"HAPPY\",  \"valence\": \"High\", \"arousal\": \"High\", \"lighting\": \"Bright\", \"motion\": \"Dynamic\"},\n",
        "    2: {\"name\": \"HORROR\", \"valence\": \"Low\",  \"arousal\": \"High\", \"lighting\": \"Dark\",   \"motion\": \"Chaotic\"},\n",
        "    3: {\"name\": \"CALM\",   \"valence\": \"High\", \"arousal\": \"Low\",  \"lighting\": \"Soft\",   \"motion\": \"Slow\"}\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# GENERIC NO-HUMAN PROMPTS\n",
        "# ------------------------------------------------------------\n",
        "emotion_prompts = {\n",
        "    0: \"empty grey room, dull lighting, silent atmosphere, no humans\",\n",
        "    1: \"bright sunlight, vivid colors, energetic light, no humans\",\n",
        "    2: \"dark abandoned corridor, fog, sharp shadows, no humans\",\n",
        "    3: \"peaceful lake at sunset, soft light, calm water, no humans\"\n",
        "}\n",
        "\n",
        "negative_prompt = \"people, humans, face, abstract, noise, glitch, distorted, blurry\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD STABLE DIFFUSION\n",
        "# ------------------------------------------------------------\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# MAIN GENERATION LOOP\n",
        "# ------------------------------------------------------------\n",
        "model.eval()\n",
        "global_window_index = 0\n",
        "num_samples = 4\n",
        "shown = 0\n",
        "\n",
        "for Xb, yb in test_loader:\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)):\n",
        "\n",
        "        if shown >= num_samples:\n",
        "            break\n",
        "\n",
        "        # ----------------------------\n",
        "        # 1. GET EEG SAMPLE\n",
        "        # ----------------------------\n",
        "        x = Xb[i:i+1]                      # (1, 8, T)\n",
        "        raw_eeg = x.squeeze(0).cpu().numpy()  # (8, T)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 2. EEG → EMOTION PREDICTION\n",
        "        # ----------------------------\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        factors = emotion_factors[pred_class]\n",
        "        prompt  = emotion_prompts[pred_class]\n",
        "\n",
        "        # ----------------------------\n",
        "        # 3. GENERATE IMAGE\n",
        "        # ----------------------------\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=45,\n",
        "            guidance_scale=9.0,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # ----------------------------\n",
        "        # 4. GRAD-CAM\n",
        "        # ----------------------------\n",
        "        cam = gradcam.generate_cam(x, pred_class)   # (time,)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 5. TIME-SEGMENT VALUES\n",
        "        # ----------------------------\n",
        "        time_energy = cam # cam is already the 1D time energy, no need for .mean(axis=0)\n",
        "        T = len(time_energy)\n",
        "\n",
        "        early_val  = time_energy[:T//3].mean() if T//3 > 0 else 0.0\n",
        "        middle_val = time_energy[T//3:2*T//3].mean() if (2*T//3 - T//3) > 0 else 0.0\n",
        "        late_val   = time_energy[2*T//3:].mean() if (T - 2*T//3) > 0 else 0.0\n",
        "\n",
        "        # ----------------------------\n",
        "        # 6. DOMINANT TIME + CHANNEL VALUES\n",
        "        # ----------------------------\n",
        "        t_idx = np.argmax(time_energy)\n",
        "\n",
        "        channel_values = raw_eeg[:, t_idx]\n",
        "        channel_values = channel_values / (np.max(np.abs(channel_values)) + 1e-8)\n",
        "\n",
        "        channel_text = (\n",
        "            f\"FC5: {channel_values[0]:.2f} | FC6: {channel_values[1]:.2f} | \"\n",
        "            f\"O1: {channel_values[2]:.2f} | O2: {channel_values[3]:.2f} |\\n\"\n",
        "            f\"P7: {channel_values[4]:.2f} | P8: {channel_values[5]:.2f} | \"\n",
        "            f\"T7: {channel_values[6]:.2f} | T8: {channel_values[7]:.2f}\"\n",
        "        )\n",
        "\n",
        "        # ----------------------------\n",
        "        # 7. FINAL BOTTOM PANEL TEXT (YOUR FORMAT)\n",
        "        # ----------------------------\n",
        "        bottom_text = (\n",
        "            f\"Emotion: {factors['name']}\\n\\n\"\n",
        "            f\"EEG Window Index: {global_window_index}\\n\\n\"\n",
        "            f\"Time-Segment Influence:\\n\"\n",
        "            f\"Early EEG  = {early_val:.3f}\\n\"\n",
        "            f\"Middle EEG = {middle_val:.3f}\\n\"\n",
        "            f\"Late EEG   = {late_val:.3f}\\n\\n\"\n",
        "            f\"{channel_text}\\n\\n\"\n",
        "            f\"Valence: {factors['valence']} | \"\n",
        "            f\"Arousal: {factors['arousal']} | \"\n",
        "            f\"Lighting: {factors['lighting']} | \"\n",
        "            f\"Motion: {factors['motion']}\"\n",
        "        )\n",
        "\n",
        "        # ----------------------------\n",
        "        # 8. DRAW CLEAR BOTTOM PANEL\n",
        "        # ----------------------------\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        try:\n",
        "            font_text = ImageFont.truetype(\n",
        "                \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 16\n",
        "            )\n",
        "        except:\n",
        "            font_text = ImageFont.load_default()\n",
        "\n",
        "        draw.rectangle((0, 300, 512, 512), fill=(0, 0, 0))\n",
        "        draw.text((15, 310), bottom_text, fill=(255, 255, 255), font=font_text)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 9. SAVE & DISPLAY\n",
        "        # ----------------------------\n",
        "        fname = f\"eeg_final_output_{shown}.png\"\n",
        "        image.save(fname)\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        shown += 1\n",
        "        global_window_index += 1\n",
        "\n",
        "    if shown >= num_samples:\n",
        "        break\n"
      ],
      "metadata": {
        "id": "HsbPOCWWQzR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 1. LOAD EEG LABELS\n",
        "# ------------------------------------------------------------\n",
        "eeg_labels = np.load(\"eeg_emotion_labels.npy\")\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 2. LOAD STABLE DIFFUSION\n",
        "# ------------------------------------------------------------\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 3. EMOTION NAMES\n",
        "# ------------------------------------------------------------\n",
        "emotion_names = {\n",
        "    0: \"boring\",\n",
        "    1: \"happy\",\n",
        "    2: \"horror\",\n",
        "    3: \"calm\"\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 4. GENERIC PSYCHOLOGICAL SCENE PROMPTS (NO HUMANS)\n",
        "# ------------------------------------------------------------\n",
        "emotion_prompts = {\n",
        "    0: \"low arousal, low valence environment, empty grey room, dull light, no movement, flat textures, silent atmosphere, no humans\",\n",
        "\n",
        "    1: \"high arousal, high valence environment, bright sunlight, vivid colors, moving clouds, energetic lighting, vivid nature scene, no humans\",\n",
        "\n",
        "    2: \"high arousal, low valence environment, dark abandoned corridor, thick fog, sharp shadows, cold colors, threatening atmosphere, no humans\",\n",
        "\n",
        "    3: \"low arousal, high valence environment, peaceful lake at sunset, smooth water, soft pastel colors, slow clouds, quiet atmosphere, no humans\"\n",
        "}\n",
        "\n",
        "negative_prompt = \"people, humans, face, body, crowd, portrait, abstract, texture, noise, mosaic, glitch, distorted, blurry, low quality\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 5. EEG TIME-SEGMENT EXPLANATION FUNCTION (from b5edc580)\n",
        "# ------------------------------------------------------------\n",
        "def explain_by_time_segment(cam_map):\n",
        "    \"\"\"\n",
        "    cam_map shape: (time_steps,)\n",
        "    \"\"\"\n",
        "    time_energy = cam_map # cam_map is already a 1D array of temporal importance\n",
        "\n",
        "    T = len(time_energy)\n",
        "    early = time_energy[:T//3].mean() if T//3 > 0 else 0.0\n",
        "    mid   = time_energy[T//3:2*T//3].mean() if (2*T//3 - T//3) > 0 else 0.0\n",
        "    late  = time_energy[2*T//3:].mean() if (T - 2*T//3) > 0 else 0.0\n",
        "\n",
        "    segments = {\n",
        "        \"early\": early,\n",
        "        \"middle\": mid,\n",
        "        \"late\": late\n",
        "    }\n",
        "\n",
        "    dominant = max(segments, key=segments.get)\n",
        "\n",
        "    return dominant, segments\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# 6. ONE-BY-ONE EEG → IMAGE + TIME-BASED EXPLANATION\n",
        "# ------------------------------------------------------------\n",
        "model.eval()\n",
        "\n",
        "sample_count = 4\n",
        "shown = 0\n",
        "\n",
        "for Xb, yb in test_loader:\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)):\n",
        "\n",
        "        if shown >= sample_count:\n",
        "            break\n",
        "\n",
        "        x = Xb[i:i+1]\n",
        "        raw_eeg_window = x.squeeze(0).cpu().numpy()   # (channels x time)\n",
        "\n",
        "        # -------- EEG → Emotion Prediction --------\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        emotion_name = emotion_names[pred_class]\n",
        "        prompt = emotion_prompts[pred_class]\n",
        "\n",
        "        # -------- EEG → GENERIC EMOTION SCENE --------\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=45,\n",
        "            guidance_scale=9.0,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # -------- TIME-BASED EXPLAINABILITY (Grad-CAM) --------\n",
        "        # This part requires a fresh backward pass on 'x' to generate CAM, so enable grad tracking for 'x'\n",
        "        x.requires_grad_(True)\n",
        "        cam = gradcam.generate_cam(x, pred_class)   # shape (time,)\n",
        "        dominant_segment, segment_scores = explain_by_time_segment(cam)\n",
        "        x.requires_grad_(False) # Disable grad tracking for 'x' after CAM calculation\n",
        "\n",
        "        # -------- CHANNEL-BASED EXPLAINABILITY (Saliency Map) --------\n",
        "        # Re-enable grad tracking for 'x' for saliency map calculation if needed, or pass it directly if already computed\n",
        "        x_for_saliency = Xb[i:i+1].clone().detach().requires_grad_(True)\n",
        "        logits_saliency = model(x_for_saliency)\n",
        "        one_hot_saliency = torch.zeros_like(logits_saliency).scatter_(1, torch.tensor([pred_class]).to(device).unsqueeze(-1), 1)\n",
        "        logits_saliency.backward(gradient=one_hot_saliency, retain_graph=False)\n",
        "        gradients = x_for_saliency.grad.abs()\n",
        "        channel_importance = gradients.mean(dim=2).squeeze().cpu().numpy() # shape (8,)\n",
        "        channel_importance = channel_importance / (channel_importance.max() + 1e-8)\n",
        "\n",
        "        # -------- DOMINANT TIME INDEX for RAW EEG Plot --------\n",
        "        t_idx, t_val = dominant_time_index(cam)\n",
        "\n",
        "        # -------- DISPLAY IMAGE + EXPLANATION --------\n",
        "        plt.figure(figsize=(6, 7))\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "        plt.title(f\"Predicted Emotion: {emotion_name.upper()}\", fontsize=14)\n",
        "\n",
        "        # Add a text box for time-segment and channel info (optional, or use plot_xai_graphs for separate plots)\n",
        "        # explain_text = (\n",
        "        #     f\"Time-Segment Influence:\\n\"\n",
        "        #     f\"Early EEG = {segment_scores['early']:.3f}\\n\"\n",
        "        #     f\"Middle EEG = {segment_scores['middle']:.3f}\\n\"\n",
        "        #     f\"Late EEG = {segment_scores['late']:.3f}\\n\\n\"\n",
        "        #     f\"Dominant Segment: {dominant_segment.upper()}\"\n",
        "        # )\n",
        "        # plt.figtext(\n",
        "        #     0.5, 0.01,\n",
        "        #     explain_text,\n",
        "        #     wrap=True,\n",
        "        #     horizontalalignment=\"center\",\n",
        "        #     fontsize=11\n",
        "        # )\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Call the XAI graphs function to display additional plots\n",
        "        plot_xai_graphs(\n",
        "            cam=cam,\n",
        "            raw_eeg=raw_eeg_window,\n",
        "            dominant_t_idx=t_idx,\n",
        "            eeg_channels=eeg_channels,\n",
        "            channel_importance_vals=channel_importance,\n",
        "            sample_id=shown\n",
        "        )\n",
        "\n",
        "        shown += 1\n",
        "\n",
        "    if shown >= sample_count:\n",
        "        break"
      ],
      "metadata": {
        "id": "HYBlKYV-UhAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5edc580"
      },
      "source": [
        "# ============================================================\n",
        "# FULL EEG → IMAGE → XAI → CLEAR BOTTOM PANEL CODE (FINAL)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import ImageDraw, ImageFont\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EEG CHANNEL NAMES\n",
        "# ------------------------------------------------------------\n",
        "eeg_channels = ['FC5','FC6','O1','O2','P7','P8','T7','T8']\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EMOTION → PSYCHOLOGICAL FACTORS\n",
        "# ------------------------------------------------------------\n",
        "emotion_factors = {\n",
        "    0: {\"name\": \"BORING\", \"valence\": \"Low\",  \"arousal\": \"Low\",  \"lighting\": \"Dull\",   \"motion\": \"Static\"},\n",
        "    1: {\"name\": \"HAPPY\",  \"valence\": \"High\", \"arousal\": \"High\", \"lighting\": \"Bright\", \"motion\": \"Dynamic\"},\n",
        "    2: {\"name\": \"HORROR\", \"valence\": \"Low\",  \"arousal\": \"High\", \"lighting\": \"Dark\",   \"motion\": \"Chaotic\"},\n",
        "    3: {\"name\": \"CALM\",   \"valence\": \"High\", \"arousal\": \"Low\",  \"lighting\": \"Soft\",   \"motion\": \"Slow\"}\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# GENERIC NO-HUMAN PROMPTS\n",
        "# ------------------------------------------------------------\n",
        "emotion_prompts = {\n",
        "    0: \"empty grey room, dull lighting, silent atmosphere, no humans\",\n",
        "    1: \"bright sunlight, vivid colors, energetic light, no humans\",\n",
        "    2: \"dark abandoned corridor, fog, sharp shadows, no humans\",\n",
        "    3: \"peaceful lake at sunset, soft light, calm water, no humans\"\n",
        "}\n",
        "\n",
        "negative_prompt = \"people, humans, face, abstract, noise, glitch, distorted, blurry\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD STABLE DIFFUSION\n",
        "# ------------------------------------------------------------\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# MAIN GENERATION LOOP\n",
        "# ------------------------------------------------------------\n",
        "model.eval()\n",
        "global_window_index = 0\n",
        "num_samples = 4\n",
        "shown = 0\n",
        "\n",
        "for Xb, yb in test_loader:\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)):\n",
        "\n",
        "        if shown >= num_samples:\n",
        "            break\n",
        "\n",
        "        # ----------------------------\n",
        "        # 1. GET EEG SAMPLE\n",
        "        # ----------------------------\n",
        "        x = Xb[i:i+1]                      # (1, 8, T)\n",
        "        raw_eeg = x.squeeze(0).cpu().numpy()  # (8, T)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 2. EEG → EMOTION PREDICTION\n",
        "        # ----------------------------\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        factors = emotion_factors[pred_class]\n",
        "        prompt  = emotion_prompts[pred_class]\n",
        "\n",
        "        # ----------------------------\n",
        "        # 3. GENERATE IMAGE\n",
        "        # ----------------------------\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=45,\n",
        "            guidance_scale=9.0,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # ----------------------------\n",
        "        # 4. GRAD-CAM\n",
        "        # ----------------------------\n",
        "        cam = gradcam.generate_cam(x, pred_class)   # (time,)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 5. TIME-SEGMENT VALUES\n",
        "        # ----------------------------\n",
        "        time_energy = cam # cam is already the 1D time energy, no need for .mean(axis=0)\n",
        "        T = len(time_energy)\n",
        "\n",
        "        early_val  = time_energy[:T//3].mean() if T//3 > 0 else 0.0\n",
        "        middle_val = time_energy[T//3:2*T//3].mean() if (2*T//3 - T//3) > 0 else 0.0\n",
        "        late_val   = time_energy[2*T//3:].mean() if (T - 2*T//3) > 0 else 0.0\n",
        "\n",
        "        # ----------------------------\n",
        "        # 6. DOMINANT TIME + CHANNEL VALUES\n",
        "        # ----------------------------\n",
        "        t_idx = np.argmax(time_energy)\n",
        "        channel_values = raw_eeg[:, t_idx]\n",
        "        channel_values = channel_values / (np.max(np.abs(channel_values)) + 1e-8)\n",
        "\n",
        "        channel_text = (\n",
        "            f\"FC5: {channel_values[0]:.2f} | FC6: {channel_values[1]:.2f} | \"\n",
        "            f\"O1: {channel_values[2]:.2f} | O2: {channel_values[3]:.2f} |\\n\"\n",
        "            f\"P7: {channel_values[4]:.2f} | P8: {channel_values[5]:.2f} | \"\n",
        "            f\"T7: {channel_values[6]:.2f} | T8: {channel_values[7]:.2f}\"\n",
        "        )\n",
        "\n",
        "        # ----------------------------\n",
        "        # 7. FINAL BOTTOM PANEL TEXT (YOUR FORMAT)\n",
        "        # ----------------------------\n",
        "        bottom_text = (\n",
        "            f\"Emotion: {factors['name']}\\n\\n\"\n",
        "            f\"EEG Window Index: {global_window_index}\\n\\n\"\n",
        "            f\"Time-Segment Influence:\\n\"\n",
        "            f\"Early EEG  = {early_val:.3f}\\n\"\n",
        "            f\"Middle EEG = {middle_val:.3f}\\n\"\n",
        "            f\"Late EEG   = {late_val:.3f}\\n\\n\"\n",
        "            f\"{channel_text}\\n\\n\"\n",
        "            f\"Valence: {factors['valence']} | \"\n",
        "            f\"Arousal: {factors['arousal']} | \"\n",
        "            f\"Lighting: {factors['lighting']} | \"\n",
        "            f\"Motion: {factors['motion']}\"\n",
        "        )\n",
        "\n",
        "        # ----------------------------\n",
        "        # 8. DRAW CLEAR BOTTOM PANEL\n",
        "        # ----------------------------\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        try:\n",
        "            font_text = ImageFont.truetype(\n",
        "                \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 16\n",
        "            )\n",
        "        except:\n",
        "            font_text = ImageFont.load_default()\n",
        "\n",
        "        draw.rectangle((0, 300, 512, 512), fill=(0, 0, 0))\n",
        "        draw.text((15, 310), bottom_text, fill=(255, 255, 255), font=font_text)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 9. SAVE & DISPLAY\n",
        "        # ----------------------------\n",
        "        fname = f\"eeg_final_output_{shown}.png\"\n",
        "        image.save(fname)\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        shown += 1\n",
        "        global_window_index += 1\n",
        "\n",
        "    if shown >= num_samples:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# FULL EEG → IMAGE → XAI → OUTLINED BOTTOM PANEL (FINAL)\n",
        "# ============================================================\n",
        "\n",
        "!pip install -q diffusers transformers accelerate safetensors\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from PIL import ImageDraw, ImageFont\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EEG CHANNEL NAMES\n",
        "# ------------------------------------------------------------\n",
        "eeg_channels = ['FC5','FC6','O1','O2','P7','P8','T7','T8']\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# EMOTION → PSYCHOLOGICAL FACTORS\n",
        "# ------------------------------------------------------------\n",
        "emotion_factors = {\n",
        "    0: {\"name\": \"BORING\", \"valence\": \"Low\",  \"arousal\": \"Low\",  \"lighting\": \"Dull\",   \"motion\": \"Static\"},\n",
        "    1: {\"name\": \"HAPPY\",  \"valence\": \"High\", \"arousal\": \"High\", \"lighting\": \"Bright\", \"motion\": \"Dynamic\"},\n",
        "    2: {\"name\": \"HORROR\", \"valence\": \"Low\",  \"arousal\": \"High\", \"lighting\": \"Dark\",   \"motion\": \"Chaotic\"},\n",
        "    3: {\"name\": \"CALM\",   \"valence\": \"High\", \"arousal\": \"Low\",  \"lighting\": \"Soft\",   \"motion\": \"Slow\"}\n",
        "}\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# GENERIC NO-HUMAN PROMPTS\n",
        "# ------------------------------------------------------------\n",
        "emotion_prompts = {\n",
        "    0: \"empty grey room, dull lighting, silent atmosphere, no humans\",\n",
        "    1: \"bright sunlight, vivid colors, energetic environment, no humans\",\n",
        "    2: \"dark abandoned corridor, fog, sharp shadows, no humans\",\n",
        "    3: \"peaceful lake at sunset, soft light, calm water, no humans\"\n",
        "}\n",
        "\n",
        "negative_prompt = \"people, humans, face, abstract, noise, glitch, distorted, blurry\"\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# LOAD STABLE DIFFUSION\n",
        "# ------------------------------------------------------------\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    torch_dtype=torch.float16,\n",
        "    safety_checker=None\n",
        ").to(device)\n",
        "\n",
        "# ------------------------------------------------------------\n",
        "# MAIN LOOP\n",
        "# ------------------------------------------------------------\n",
        "model.eval()\n",
        "\n",
        "global_window_index = 0\n",
        "num_samples = 4\n",
        "snown = 0\n",
        "\n",
        "for Xb, yb in test_loader:\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)):\n",
        "\n",
        "        if snown >= num_samples:\n",
        "            break\n",
        "\n",
        "        # ----------------------------\n",
        "        # 1. GET EEG SAMPLE\n",
        "        # ----------------------------\n",
        "        x = Xb[i:i+1]                          # (1, 8, T)\n",
        "        raw_eeg = x.squeeze(0).cpu().numpy()  # (8, T)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 2. EEG → EMOTION\n",
        "        # ----------------------------\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        factors = emotion_factors[pred_class]\n",
        "        prompt  = emotion_prompts[pred_class]\n",
        "\n",
        "        # ----------------------------\n",
        "        # 3. GENERATE IMAGE\n",
        "        # ----------------------------\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=45,\n",
        "            guidance_scale=9.0,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # ----------------------------\n",
        "        # 4. GRAD-CAM\n",
        "        # ----------------------------\n",
        "        cam = gradcam.generate_cam(x, pred_class)   # (time,)\n",
        "\n",
        "        # ----------------------------\n",
        "        # 5. TIME-SEGMENT VALUES\n",
        "        # ----------------------------\n",
        "        time_energy = cam # cam is already the 1D time energy\n",
        "        T = len(time_energy)\n",
        "\n",
        "        early_val  = time_energy[:T//3].mean() if T//3 > 0 else 0.0\n",
        "        middle_val = time_energy[T//3:2*T//3].mean() if (2*T//3 - T//3) > 0 else 0.0\n",
        "        late_val   = time_energy[2*T//3:].mean() if (T - 2*T//3) > 0 else 0.0\n",
        "\n",
        "        # ----------------------------\n",
        "        # 6. DOMINANT TIME + CHANNEL VALUES\n",
        "        # ----------------------------\n",
        "        t_idx = np.argmax(time_energy)\n",
        "\n",
        "        channel_values = raw_eeg[:, t_idx]\n",
        "        channel_values = channel_values / (np.max(np.abs(channel_values)) + 1e-8)\n",
        "\n",
        "        channel_text = (\n",
        "            f\"FC5: {channel_values[0]:.2f} | FC6: {channel_values[1]:.2f} | \"\n",
        "            f\"O1: {channel_values[2]:.2f} | O2: {channel_values[3]:.2f} |\\n\"\n",
        "            f\"P7: {channel_values[4]:.2f} | P8: {channel_values[5]:.2f} | \"\n",
        "            f\"T7: {channel_values[6]:.2f} | T8: {channel_values[7]:.2f}\"\n",
        "        )\n",
        "\n",
        "        # ----------------------------\n",
        "        # 7. FINAL BOTTOM PANEL TEXT\n",
        "        # ----------------------------\n",
        "        bottom_text = (\n",
        "            f\"Emotion: {factors['name']}\\n\\n\"\n",
        "            f\"EEG Window Index: {global_window_index}\\n\\n\"\n",
        "            f\"Time-Segment Influence:\\n\"\n",
        "            f\"Early EEG  = {early_val:.3f}\\n\"\n",
        "            f\"Middle EEG = {middle_val:.3f}\\n\"\n",
        "            f\"Late EEG   = {late_val:.3f}\\n\\n\"\n",
        "            f\"{channel_text}\\n\\n\"\n",
        "            f\"Valence: {factors['valence']} | \"\n",
        "            f\"Arousal: {factors['arousal']} | \"\n",
        "            f\"Lighting: {factors['lighting']} | \"\n",
        "            f\"Motion: {factors['motion']}\"\n",
        "        )\n",
        "\n",
        "        # ----------------------------\n",
        "        # 8. OUTLINED BOTTOM PANEL\n",
        "        # ----------------------------\n",
        "        draw = ImageDraw.Draw(image)\n",
        "\n",
        "        try:\n",
        "            font_text = ImageFont.truetype(\n",
        "                \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", 22\n",
        "            )\n",
        "        except:\n",
        "            font_text = ImageFont.load_default()\n",
        "\n",
        "        # Tall bottom panel\n",
        "        draw.rectangle((0, 260, 512, 512), fill=(0, 0, 0))\n",
        "\n",
        "        x0, y0 = 20, 275\n",
        "\n",
        "        # ---- BLACK OUTLINE ----\n",
        "        for dx in [-1, 0, 1]:\n",
        "            for dy in [-1, 0, 1]:\n",
        "                draw.multiline_text(\n",
        "                    (x0 + dx, y0 + dy),\n",
        "                    bottom_text,\n",
        "                    fill=(0, 0, 0),\n",
        "                    font=font_text,\n",
        "                    spacing=8,\n",
        "                    align=\"left\"\n",
        "                )\n",
        "\n",
        "        # ---- WHITE MAIN TEXT ----\n",
        "        draw.multiline_text(\n",
        "            (x0, y0),\n",
        "            bottom_text,\n",
        "            fill=(255, 255, 255),\n",
        "            font=font_text,\n",
        "            spacing=8,\n",
        "            align=\"left\"\n",
        "        )\n",
        "\n",
        "        # ----------------------------\n",
        "        # 9. SAVE & DISPLAY\n",
        "        # ----------------------------\n",
        "        fname = f\"eeg_final_outlined_{snown}.png\"\n",
        "        image.save(fname)\n",
        "\n",
        "        plt.figure(figsize=(5, 5))\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "\n",
        "        snown += 1\n",
        "        global_window_index += 1\n",
        "\n",
        "    if snown >= num_samples:\n",
        "        break"
      ],
      "metadata": {
        "id": "Rng5MAcYhI-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# history = dict containing training logs\n",
        "plt.figure()\n",
        "plt.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
        "plt.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CDdIgblNiGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30dcbd94"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy import signal\n",
        "import numpy as np\n",
        "\n",
        "# --- Configuration for Spectrogram ---\n",
        "# Assuming a sampling rate of 256 Hz for the EEG data\n",
        "# (This is a common sampling rate for EEG research and aligns with typical windowing)\n",
        "FS = 256  # Sampling frequency (Hz)\n",
        "\n",
        "# Frequency bands\n",
        "FREQ_BANDS = {\n",
        "    'Delta': (1, 4),\n",
        "    'Theta': (4, 8),\n",
        "    'Alpha': (8, 12),\n",
        "    'Beta': (12, 30)\n",
        "}\n",
        "\n",
        "# --- Get an EEG sample for analysis ---\n",
        "# Taking the first sample from the test_loader (already normalized)\n",
        "# Ensure X_sample has been converted back to (time, channels) for this plot\n",
        "# The current X_sample from `l-uvTyNWPJyC` is (1, channels, time), let's reshape for plotting.\n",
        "\n",
        "# Fetch one batch from the test loader\n",
        "for Xb_test, yb_test in test_loader:\n",
        "    # Take the first sample from the batch\n",
        "    eeg_sample = Xb_test[0].cpu().numpy() # Shape will be (channels, time)\n",
        "    true_label = yb_test[0].item()\n",
        "    break\n",
        "\n",
        "# Reshape eeg_sample to (time, channels) for easier processing if needed for some functions,\n",
        "# but for channel-wise spectrogram, (channels, time) is fine.\n",
        "# Let's verify the shape and use it as is if compatible.\n",
        "# eeg_sample.shape: (8, 512)\n",
        "\n",
        "print(f\"Analyzing EEG sample for true emotion label: {true_label}\")\n",
        "print(f\"EEG sample shape: {eeg_sample.shape} (channels, time)\")\n",
        "\n",
        "# --- Plotting the Spectrogram for each channel ---\n",
        "fig, axes = plt.subplots(eeg_sample.shape[0], 1, figsize=(12, 2 * eeg_sample.shape[0]), sharex=True)\n",
        "fig.suptitle('EEG Time-Frequency Spectrogram by Channel', y=0.99, fontsize=16)\n",
        "\n",
        "for i, channel_data in enumerate(eeg_sample):\n",
        "    # Compute the spectrogram\n",
        "    f, t, Sxx = signal.spectrogram(channel_data, FS, nperseg=FS*2, noverlap=FS*1.5)\n",
        "\n",
        "    ax = axes[i]\n",
        "    im = ax.pcolormesh(t, f, 10 * np.log10(Sxx), shading='gouraud', cmap='viridis')\n",
        "    ax.set_ylabel('Frequency [Hz]')\n",
        "    ax.set_title(f'Channel: {eeg_channels[i]}')\n",
        "\n",
        "    # Highlight frequency bands\n",
        "    for band_name, (f_low, f_high) in FREQ_BANDS.items():\n",
        "        ax.axhspan(f_low, f_high, color='r', alpha=0.1, label=band_name)\n",
        "        # Add text label for band\n",
        "        ax.text(t[-1]*1.01, (f_low + f_high) / 2, band_name, color='red', va='center', ha='left', fontsize=8)\n",
        "\n",
        "    ax.set_ylim(0, 40) # Limit frequency display to relevant range\n",
        "\n",
        "fig.colorbar(im, ax=axes.ravel().tolist(), label='Power/Frequency [dB/Hz]')\n",
        "\n",
        "axes[-1].set_xlabel('Time [sec]')\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust layout to prevent suptitle overlap\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ba7d817"
      },
      "source": [
        "# Task\n",
        "Calculate the consistency between generated images and their corresponding emotion text prompts by:\n",
        "1. Loading a pre-trained CLIP Vision Model and its processor.\n",
        "2. For a subset of the test dataset, predicting the emotion for each EEG sample, generating an image using Stable Diffusion with the corresponding text prompt, and then encoding both the generated image and the text prompt into CLIP embeddings.\n",
        "3. Computing the cosine similarity between these image and text embeddings.\n",
        "4. Visualizing the distribution of these consistency scores using a histogram.\n",
        "5. Summarizing the findings regarding how well generated images align with predicted emotions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a78e0a16"
      },
      "source": [
        "## Load CLIP Model for Image Embeddings\n",
        "\n",
        "### Subtask:\n",
        "Load a pre-trained CLIP model's image encoder and processor to convert generated images into embeddings suitable for comparison with text embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fccb1308"
      },
      "source": [
        "**Reasoning**:\n",
        "To convert images into embeddings, I need to load the CLIP processor and the CLIP vision model from the `transformers` library. The `CLIPProcessor` will handle image preprocessing, and the `CLIPVisionModel` will extract the image features.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e0627db"
      },
      "source": [
        "from transformers import CLIPProcessor, CLIPVisionModel\n",
        "\n",
        "# Load pre-trained CLIP processor\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Load pre-trained CLIP vision model and move to device\n",
        "clip_vision_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "print(\"CLIP Processor and Vision Model loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "670fcd0e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output showed a warning about `use_fast` but no error, and confirmed the CLIP processor and vision model were loaded. The next step in the task is to prepare to calculate the consistency between generated images and text prompts. This involves loading the CLIP tokenizer and text encoder to convert prompts into embeddings, then iterating through the test dataset to generate images from predicted EEG emotions and encode both images and their corresponding prompts into CLIP embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e028f848"
      },
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "print(\"CLIP Tokenizer and Text Model loaded successfully.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings --- #\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        text_features = clip_text_model(**inputs).last_hidden_state\n",
        "    # CLIP text embeddings often use the pooled output or mean of last_hidden_state\n",
        "    # For comparison with image embeddings, the pooled output [CLS] token embedding is commonly used.\n",
        "    # However, for stable diffusion text encoder, it's often the last_hidden_state.mean(dim=1)\n",
        "    # Let's use the pooled_output if available, otherwise mean of last_hidden_state for consistency with common CLIP usage\n",
        "    if hasattr(clip_text_model, 'pooler_output'):\n",
        "        return clip_text_model(**inputs).pooler_output # For models that explicitly output pooled_output\n",
        "    else:\n",
        "        return text_features.mean(dim=1) # Fallback to mean if no explicit pooler_output\n",
        "\n",
        "# --- Collect embeddings for consistency calculation --- #\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "pipe.safety_checker = lambda images, **kwargs: (images, False) # Disable safety checker for faster generation\n",
        "\n",
        "count = 0\n",
        "for Xb, yb in test_loader:\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)):\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "999bfe0e"
      },
      "source": [
        "# Task\n",
        "The `TypeError: 'bool' object is not iterable` occurred because the `pipe.safety_checker` was incorrectly assigned a lambda function that returned a boolean, which the Stable Diffusion pipeline tried to iterate over. To fix this, the `safety_checker` will be explicitly set to `None` to fully disable it, allowing the image generation process to continue without errors.\n",
        "\n",
        "After this correction, I will proceed with the plan to:\n",
        "1. Load a pre-trained CLIP Vision Model and its processor (already done).\n",
        "2. Iterate through a subset of the test dataset, predict the emotion for each EEG sample, generate an image using Stable Diffusion with the corresponding text prompt, and then encode both the generated image and the text prompt into CLIP embeddings.\n",
        "3. Compute the cosine similarity between these image and text embeddings.\n",
        "4. Visualize the distribution of these consistency scores using a histogram.\n",
        "5. Summarize the findings regarding how well generated images align with predicted emotions.\n",
        "\n",
        "```python\n",
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "# (These were already loaded, but including for completeness if cell were run standalone)\n",
        "# clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "# clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "print(\"CLIP Tokenizer and Text Model loaded successfully.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings --- #\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=77 # Standard CLIP max length\n",
        "    ).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # CLIP models generally output `pooler_output` for sentence embeddings.\n",
        "        # `last_hidden_state` is the sequence of token embeddings.\n",
        "        outputs = clip_text_model(**inputs)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            return outputs.pooler_output\n",
        "        else:\n",
        "            # Fallback for models without explicit pooler_output, typically using mean of last_hidden_state\n",
        "            return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# --- Collect embeddings for consistency calculation --- #\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = [] # To store cosine similarity scores\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "\n",
        "# FIX: Set safety_checker to None to truly disable it.\n",
        "# It was already passed during from_pretrained, but if it were set to a callable,\n",
        "# this is how to disable it afterward.\n",
        "pipe.safety_checker = None\n",
        "\n",
        "count = 0\n",
        "for Xb, yb in test_loader:\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)):\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "            # Ensure the image embedding is normalized for cosine similarity if not already\n",
        "            image_embedding = image_embedding / image_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        # Ensure the text embedding is normalized for cosine similarity if not already\n",
        "        text_embedding = text_embedding / text_embedding.norm(p=2, dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        similarity = torch.cosine_similarity(image_embedding, text_embedding, dim=-1)\n",
        "        consistency_scores.append(similarity.item())\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)\n",
        "\n",
        "# Plotting the distribution of consistency scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(consistency_scores, bins=20, kde=True)\n",
        "plt.title('Distribution of Image-Emotion Consistency Scores (Cosine Similarity)')\n",
        "plt.xlabel('Cosine Similarity Score (Image Embedding vs. Text Embedding)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Summarize findings\n",
        "mean_score = np.mean(consistency_scores)\n",
        "median_score = np.median(consistency_scores)\n",
        "std_dev_score = np.std(consistency_scores)\n",
        "\n",
        "print(f\"\\nSummary of Image-Emotion Consistency Scores:\")\n",
        "print(f\"Mean Cosine Similarity: {mean_score:.4f}\")\n",
        "print(f\"Median Cosine Similarity: {median_score:.4f}\")\n",
        "print(f\"Standard Deviation: {std_dev_score:.4f}\")\n",
        "print(f\"Min Score: {np.min(consistency_scores):.4f}\")\n",
        "print(f\"Max Score: {np.max(consistency_scores):.4f}\")\n",
        "\n",
        "# Discussion of findings based on the distribution\n",
        "if mean_score > 0.7:\n",
        "    print(\"\\nThe high mean cosine similarity suggests a strong alignment between the generated images and their corresponding emotion text prompts. The Stable Diffusion model, guided by the EEG-predicted emotions, is likely producing images that effectively capture the semantic meaning of the emotion prompts.\")\n",
        "elif mean_score > 0.5:\n",
        "    print(\"\\nThe moderate mean cosine similarity indicates a reasonable alignment between the generated images and their corresponding emotion text prompts. There's a good general correlation, but there might be room for improvement in fine-tuning prompts or the adapter for stronger consistency.\")\n",
        "else:\n",
        "    print(\"\\nThe low mean cosine similarity suggests a weak alignment between the generated images and their corresponding emotion text prompts. This could indicate that either the EEG-to-CLIP adapter needs further training, the emotion prompts are not sufficiently descriptive, or the Stable Diffusion model struggles to interpret the nuances of these emotion prompts effectively.\")\n",
        "\n",
        "print(f\"\\nDetailed analysis of the histogram shows the distribution of scores. A distribution clustered towards higher values (closer to 1) indicates better consistency, while a spread towards lower values (closer to 0 or negative) suggests less alignment. The shape of the KDE (Kernel Density Estimate) also provides insight into the overall quality and reliability of the image generation process based on emotion predictions.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1fc8ab7"
      },
      "source": [
        "## Calculate Image-Emotion Consistency Scores\n",
        "\n",
        "### Subtask:\n",
        "Iterate through a subset of the test dataset, predict the emotion for each EEG sample, generate an image using Stable Diffusion with the corresponding text prompt, and then encode both the generated image and the text prompt into CLIP embeddings. Compute the cosine similarity between these image and text embeddings.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5803a5aa"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block encountered a `TypeError` due to an incorrect assignment for `pipe.safety_checker`. I will regenerate the code block, ensuring that `pipe.safety_checker` is correctly handled by explicitly setting it to `None` only if it's not already `None`. I will also add the cosine similarity calculation between the normalized image and text embeddings and store them in a new list, as per the subtask instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8313a36"
      },
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "print(\"CLIP Tokenizer and Text Model loaded successfully.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings ---\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # CLIP text models usually return `pooler_output` for sentence-level embeddings\n",
        "        # If not available, we can use the mean of `last_hidden_state`\n",
        "        outputs = clip_text_model(**inputs)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            return outputs.pooler_output\n",
        "        else:\n",
        "            # Fallback for models without explicit pooler_output (e.g., some T5 or older transformers)\n",
        "            return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# --- Collect embeddings and consistency scores for calculation ---\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "\n",
        "# Ensure pipe's safety checker is disabled for faster generation\n",
        "# It was already loaded with safety_checker=None in earlier cells,\n",
        "# so we don't need to reassign it in a way that causes errors.\n",
        "# If it were enabled, it would return (images, [bools]), but here we assume it's None.\n",
        "\n",
        "count = 0\n",
        "for Xb, yb in test_loader:\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)): # Iterate through batch\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding and normalize\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "            image_embedding_norm = F.normalize(image_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding and normalize\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        text_embedding_norm = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        # Cosine similarity is the dot product of L2-normalized vectors\n",
        "        similarity = torch.sum(image_embedding_norm * text_embedding_norm).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs and {len(consistency_scores)} consistency scores.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ff3b653"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code block failed because the `pipe.safety_checker` was set to a lambda function that returned a single boolean instead of an iterable of booleans, causing a `TypeError`. I will correct the lambda function to return a list of booleans, which aligns with the expected input format for the pipeline's internal processing of safety checker results. This will enable the stable diffusion pipeline to generate images and allow the computation of image-text consistency scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4fc95a1"
      },
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "clip_text_model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "print(\"CLIP Tokenizer and Text Model loaded successfully.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings ---\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # CLIP text models usually return `pooler_output` for sentence-level embeddings\n",
        "        # If not available, we can use the mean of `last_hidden_state`\n",
        "        outputs = clip_text_model(**inputs)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            return outputs.pooler_output\n",
        "        else:\n",
        "            # Fallback for models without explicit pooler_output (e.g., some T5 or older transformers)\n",
        "            return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# --- Collect embeddings and consistency scores for calculation ---\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "\n",
        "# FIX: Correctly set pipe.safety_checker to return an iterable (list of bools)\n",
        "# This prevents the TypeError when the pipeline attempts to iterate over the safety checker's output.\n",
        "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "count = 0\n",
        "for Xb, yb in test_loader:\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)): # Iterate through batch\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding and normalize\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "            image_embedding_norm = F.normalize(image_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding and normalize\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        text_embedding_norm = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        # Cosine similarity is the dot product of L2-normalized vectors\n",
        "        similarity = torch.sum(image_embedding_norm * text_embedding_norm).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs and {len(consistency_scores)} consistency scores.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6c9b861"
      },
      "source": [
        "**Reasoning**:\n",
        "The `RuntimeError` regarding tensor size mismatch (768 vs 512) indicates that the CLIP image and text embeddings being compared have different dimensions. This likely stems from using `clip-vit-base-patch32` (which typically outputs 512-dim embeddings) for the image and text encoders, while Stable Diffusion v1.5's internal text encoder (which generates the initial text embeddings for prompt processing) is usually based on `clip-vit-large-patch14` and outputs 768-dim embeddings. To resolve this inconsistency and ensure that both image and text embeddings used for similarity calculation are of the same dimension, I will explicitly load all CLIP components (`CLIPProcessor`, `CLIPVisionModel`, `CLIPTokenizer`, `CLIPTextModel`) from the `openai/clip-vit-large-patch14` model. This model outputs 768-dimensional embeddings for both modalities, making them compatible for cosine similarity calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92f9c106"
      },
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPProcessor, CLIPVisionModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model name for CLIP components to ensure consistent embedding dimensions\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_text_model = CLIPTextModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "# Load pre-trained CLIP processor and vision model\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_vision_model = CLIPVisionModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "print(f\"CLIP components ({CLIP_MODEL_NAME}) loaded successfully with consistent embedding dimensions.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings ---\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # CLIP text models usually return `pooler_output` for sentence-level embeddings\n",
        "        # If not available, we can use the mean of `last_hidden_state`\n",
        "        outputs = clip_text_model(**inputs)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            return outputs.pooler_output\n",
        "        else:\n",
        "            # Fallback for models without explicit pooler_output\n",
        "            return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# --- Collect embeddings and consistency scores for calculation ---\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "\n",
        "# FIX: Correctly set pipe.safety_checker to return an iterable (list of bools)\n",
        "# This prevents the TypeError when the pipeline attempts to iterate over the safety checker's output.\n",
        "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "count = 0\n",
        "for Xb, yb in test_loader:\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)): # Iterate through batch\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding and normalize\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "            image_embedding_norm = F.normalize(image_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding and normalize\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        text_embedding_norm = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        # Cosine similarity is the dot product of L2-normalized vectors\n",
        "        similarity = torch.sum(image_embedding_norm * text_embedding_norm).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs and {len(consistency_scores)} consistency scores.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87c5802d"
      },
      "source": [
        "# Task\n",
        "The previous attempt to calculate image-emotion consistency scores failed with a `RuntimeError` indicating a tensor size mismatch (1024 vs 768) when using `openai/clip-vit-large-patch14`. This suggests that `CLIPVisionModel` and `CLIPTextModel` from `large-patch14` do not produce `pooler_output` embeddings of the same dimension (vision is 1024, text is 768).\n",
        "\n",
        "To resolve this dimension inconsistency, I will modify the code to use `openai/clip-vit-base-patch32` for all CLIP components (`CLIPProcessor`, `CLIPVisionModel`, `CLIPTokenizer`, `CLIPTextModel`) for the consistency calculation. This model is known to produce consistent 512-dimensional `pooler_output` embeddings for both image and text, which is crucial for direct cosine similarity comparisons.\n",
        "\n",
        "I will also include print statements to explicitly show the dimensions of the generated image embeddings and text embeddings just before the similarity calculation, to verify that they are indeed consistent (512-dimensional).\n",
        "\n",
        "```python\n",
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPProcessor, CLIPVisionModel\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model name for CLIP components to ensure consistent embedding dimensions (512-dim for base-patch32)\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_text_model = CLIPTextModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "# Load pre-trained CLIP processor and vision model\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_vision_model = CLIPVisionModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "print(f\"CLIP components ({CLIP_MODEL_NAME}) loaded successfully.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings ---\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # CLIP text models usually return `pooler_output` for sentence-level embeddings\n",
        "        # If not available, we can use the mean of `last_hidden_state`\n",
        "        outputs = clip_text_model(**inputs)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            return outputs.pooler_output\n",
        "        else:\n",
        "            # Fallback for models without explicit pooler_output\n",
        "            return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# --- Collect embeddings and consistency scores for calculation ---\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "\n",
        "# Ensure pipe's safety checker is correctly set to return an iterable (list of bools)\n",
        "# This prevents the TypeError encountered in previous runs.\n",
        "# Note: The 'pipe' variable is from Stable Diffusion, not the CLIP model loaded here.\n",
        "if pipe.safety_checker is not None: # Check if it exists before trying to modify\n",
        "    pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "\n",
        "count = 0\n",
        "for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)): # Iterate through batch\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding and normalize\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "            image_embedding_norm = F.normalize(image_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding and normalize\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        text_embedding_norm = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "        # Verify dimensions before calculation\n",
        "        if batch_idx == 0 and i == 0: # Print shapes only for the first sample\n",
        "            print(f\"DEBUG: Image embedding shape: {image_embedding_norm.shape}\")\n",
        "            print(f\"DEBUG: Text embedding shape: {text_embedding_norm.shape}\")\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        # Cosine similarity is the dot product of L2-normalized vectors\n",
        "        similarity = torch.sum(image_embedding_norm * text_embedding_norm).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs and {len(consistency_scores)} consistency scores.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)\n",
        "\n",
        "# Plotting the distribution of consistency scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(consistency_scores, bins=20, kde=True)\n",
        "plt.title('Distribution of Image-Emotion Consistency Scores (Cosine Similarity)')\n",
        "plt.xlabel('Cosine Similarity Score (Image Embedding vs. Text Embedding)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Summarize findings\n",
        "mean_score = np.mean(consistency_scores)\n",
        "median_score = np.median(consistency_scores)\n",
        "std_dev_score = np.std(consistency_scores)\n",
        "\n",
        "print(f\"\\nSummary of Image-Emotion Consistency Scores:\")\n",
        "print(f\"Mean Cosine Similarity: {mean_score:.4f}\")\n",
        "print(f\"Median Cosine Similarity: {median_score:.4f}\")\n",
        "print(f\"Standard Deviation: {std_dev_score:.4f}\")\n",
        "print(f\"Min Score: {np.min(consistency_scores):.4f}\")\n",
        "print(f\"Max Score: {np.max(consistency_scores):.4f}\")\n",
        "\n",
        "# Discussion of findings based on the distribution\n",
        "if mean_score > 0.7:\n",
        "    print(\"\\nThe high mean cosine similarity suggests a strong alignment between the generated images and their corresponding emotion text prompts. The Stable Diffusion model, guided by the EEG-predicted emotions, is likely producing images that effectively capture the semantic meaning of the emotion prompts.\")\n",
        "elif mean_score > 0.5:\n",
        "    print(\"\\nThe moderate mean cosine similarity indicates a reasonable alignment between the generated images and their corresponding emotion text prompts. There's a good general correlation, but there might be room for improvement in fine-tuning prompts or the adapter for stronger consistency.\")\n",
        "else:\n",
        "    print(\"\\nThe low mean cosine similarity suggests a weak alignment between the generated images and their corresponding emotion text prompts. This could indicate that either the EEG-to-CLIP adapter needs further training, the emotion prompts are not sufficiently descriptive, or the Stable Diffusion model struggles to interpret the nuances of these emotion prompts effectively.\")\n",
        "\n",
        "print(f\"\\nDetailed analysis of the histogram shows the distribution of scores. A distribution clustered towards higher values (closer to 1) indicates better consistency, while a spread towards lower values (closer to 0 or negative) suggests less alignment. The shape of the KDE (Kernel Density Estimate) also provides insight into the overall quality and reliability of the image generation process based on emotion predictions.\")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6aa08547"
      },
      "source": [
        "## Load CLIP Model for Image Embeddings\n",
        "\n",
        "### Subtask:\n",
        "Reload all CLIP components (`CLIPProcessor`, `CLIPVisionModel`, `CLIPTokenizer`, `CLIPTextModel`) using `openai/clip-vit-large-patch14` to ensure consistent 768-dimensional embeddings for both modalities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "924c7b8e"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to load CLIP models resulted in a size mismatch error because the image and text embeddings had different dimensions. To fix this, I need to reload all CLIP components using a consistent model, `openai/clip-vit-large-patch14`, which provides 768-dimensional embeddings for both modalities, ensuring compatibility for cosine similarity calculations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbb7e65d"
      },
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPProcessor, CLIPVisionModel\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Model name for CLIP components to ensure consistent embedding dimensions\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_text_model = CLIPTextModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "# Load pre-trained CLIP processor and vision model\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_vision_model = CLIPVisionModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "print(f\"CLIP components ({CLIP_MODEL_NAME}) loaded successfully with consistent embedding dimensions.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings ---\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # CLIP text models usually return `pooler_output` for sentence-level embeddings\n",
        "        # If not available, we can use the mean of `last_hidden_state`\n",
        "        outputs = clip_text_model(**inputs)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            return outputs.pooler_output\n",
        "        else:\n",
        "            # Fallback for models without explicit pooler_output\n",
        "            return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# --- Collect embeddings and consistency scores for calculation ---\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "\n",
        "# FIX: Correctly set pipe.safety_checker to return an iterable (list of bools)\n",
        "# This prevents the TypeError when the pipeline attempts to iterate over the safety checker's output.\n",
        "pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "count = 0\n",
        "for Xb, yb in test_loader:\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)): # Iterate through batch\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding and normalize\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "            image_embedding_norm = F.normalize(image_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding and normalize\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        text_embedding_norm = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        # Cosine similarity is the dot product of L2-normalized vectors\n",
        "        similarity = torch.sum(image_embedding_norm * text_embedding_norm).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs and {len(consistency_scores)} consistency scores.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94bd901a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to calculate image-emotion consistency scores failed due to a tensor size mismatch between the CLIP image and text embeddings (1024 vs 768) when using `openai/clip-vit-large-patch14`. To resolve this, I will reload all CLIP components using `openai/clip-vit-base-patch32`, which provides consistent 512-dimensional `pooler_output` embeddings for both modalities, ensuring compatibility for cosine similarity calculations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "10aaa8f4"
      },
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPProcessor, CLIPVisionModel\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model name for CLIP components to ensure consistent embedding dimensions (512-dim for base-patch32)\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
        "\n",
        "# Load pre-trained CLIP tokenizer and text model\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_text_model = CLIPTextModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "# Load pre-trained CLIP processor and vision model\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_vision_model = CLIPVisionModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "\n",
        "print(f\"CLIP components ({CLIP_MODEL_NAME}) loaded successfully.\")\n",
        "\n",
        "# --- Helper function to get CLIP text embeddings ---\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # CLIP text models usually return `pooler_output` for sentence-level embeddings\n",
        "        # If not available, we can use the mean of `last_hidden_state`\n",
        "        outputs = clip_text_model(**inputs)\n",
        "        if hasattr(outputs, 'pooler_output') and outputs.pooler_output is not None:\n",
        "            return outputs.pooler_output\n",
        "        else:\n",
        "            # Fallback for models without explicit pooler_output\n",
        "            return outputs.last_hidden_state.mean(dim=1)\n",
        "\n",
        "# --- Collect embeddings and consistency scores for calculation ---\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_vision_model.eval()\n",
        "clip_text_model.eval()\n",
        "\n",
        "# Ensure pipe's safety checker is correctly set to return an iterable (list of bools)\n",
        "# This prevents the TypeError encountered in previous runs.\n",
        "# Note: The 'pipe' variable is from Stable Diffusion, not the CLIP model loaded here.\n",
        "if pipe.safety_checker is not None: # Check if it exists before trying to modify\n",
        "    pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "\n",
        "count = 0\n",
        "for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)): # Iterate through batch\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding and normalize\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = clip_vision_model(**image_inputs).pooler_output\n",
        "            image_embedding_norm = F.normalize(image_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding and normalize\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        text_embedding_norm = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "        # Verify dimensions before calculation\n",
        "        if batch_idx == 0 and i == 0: # Print shapes only for the first sample\n",
        "            print(f\"DEBUG: Image embedding shape: {image_embedding_norm.shape}\")\n",
        "            print(f\"DEBUG: Text embedding shape: {text_embedding_norm.shape}\")\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        # Cosine similarity is the dot product of L2-normalized vectors\n",
        "        similarity = torch.sum(image_embedding_norm * text_embedding_norm).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs and {len(consistency_scores)} consistency scores.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)\n",
        "\n",
        "# Plotting the distribution of consistency scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(consistency_scores, bins=20, kde=True)\n",
        "plt.title('Distribution of Image-Emotion Consistency Scores (Cosine Similarity)')\n",
        "plt.xlabel('Cosine Similarity Score (Image Embedding vs. Text Embedding)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Summarize findings\n",
        "mean_score = np.mean(consistency_scores)\n",
        "median_score = np.median(consistency_scores)\n",
        "std_dev_score = np.std(consistency_scores)\n",
        "\n",
        "print(f\"\\nSummary of Image-Emotion Consistency Scores:\")\n",
        "print(f\"Mean Cosine Similarity: {mean_score:.4f}\")\n",
        "print(f\"Median Cosine Similarity: {median_score:.4f}\")\n",
        "print(f\"Standard Deviation: {std_dev_score:.4f}\")\n",
        "print(f\"Min Score: {np.min(consistency_scores):.4f}\")\n",
        "print(f\"Max Score: {np.max(consistency_scores):.4f}\")\n",
        "\n",
        "# Discussion of findings based on the distribution\n",
        "if mean_score > 0.7:\n",
        "    print(\"\\nThe high mean cosine similarity suggests a strong alignment between the generated images and their corresponding emotion text prompts. The Stable Diffusion model, guided by the EEG-predicted emotions, is likely producing images that effectively capture the semantic meaning of the emotion prompts.\")\n",
        "elif mean_score > 0.5:\n",
        "    print(\"\\nThe moderate mean cosine similarity indicates a reasonable alignment between the generated images and their corresponding emotion text prompts. There's a good general correlation, but there might be room for improvement in fine-tuning prompts or the adapter for stronger consistency.\")\n",
        "else:\n",
        "    print(\"\\nThe low mean cosine similarity suggests a weak alignment between the generated images and their corresponding emotion text prompts. This could indicate that either the EEG-to-CLIP adapter needs further training, the emotion prompts are not sufficiently descriptive, or the Stable Diffusion model struggles to interpret the nuances of these emotion prompts effectively.\")\n",
        "\n",
        "print(f\"\\nDetailed analysis of the histogram shows the distribution of scores. A distribution clustered towards higher values (closer to 1) indicates better consistency, while a spread towards lower values (closer to 0 or negative) suggests less alignment. The shape of the KDE (Kernel Density Estimate) also provides insight into the overall quality and reliability of the image generation process based on emotion predictions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d52cbbb3"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous `RuntimeError` for tensor size mismatch (1024 vs 768) indicates that `CLIPVisionModel` and `CLIPTextModel` from `openai/clip-vit-large-patch14` provide `pooler_output` of different dimensions. To correctly compare image and text embeddings, I need to use the dedicated feature extraction methods `clip_model.get_image_features()` and `clip_model.get_text_features()`, which project both modalities into a consistent shared latent space (768 dimensions for `large-patch14`). I will load the overarching `CLIPModel` to ensure access to these methods and guarantee consistent feature dimensions for cosine similarity calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8941af12"
      },
      "source": [
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPProcessor, CLIPVisionModel, CLIPModel\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Model name for CLIP components to ensure consistent embedding dimensions\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "# Load the full CLIP model, processor, and tokenizer\n",
        "clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(CLIP_MODEL_NAME)\n",
        "\n",
        "print(f\"CLIP components ({CLIP_MODEL_NAME}) loaded successfully with consistent embedding dimensions for features.\")\n",
        "\n",
        "# --- Helper function to get CLIP text features --- #\n",
        "def get_clip_text_embedding(prompt):\n",
        "    inputs = clip_tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        # Use the dedicated method to get projected text features\n",
        "        text_features = clip_model.get_text_features(**inputs)\n",
        "    return text_features\n",
        "\n",
        "# --- Collect embeddings and consistency scores for calculation --- #\n",
        "image_embeddings_list = []\n",
        "text_embeddings_list = []\n",
        "consistency_scores = []\n",
        "\n",
        "num_samples_to_evaluate = 50 # Evaluate a subset for efficiency\n",
        "\n",
        "model.eval()\n",
        "clip_model.eval() # Ensure the full CLIP model is in evaluation mode\n",
        "\n",
        "# Ensure pipe's safety checker is correctly set to return an iterable (list of bools)\n",
        "# This prevents the TypeError encountered in previous runs.\n",
        "# Note: The 'pipe' variable is from Stable Diffusion, not the CLIP model loaded here.\n",
        "if pipe.safety_checker is not None: # Check if it exists before trying to modify\n",
        "    pipe.safety_checker = lambda images, **kwargs: (images, [False] * len(images))\n",
        "\n",
        "\n",
        "count = 0\n",
        "for batch_idx, (Xb, yb) in enumerate(test_loader):\n",
        "    if count >= num_samples_to_evaluate:\n",
        "        break\n",
        "\n",
        "    Xb = Xb.to(device)\n",
        "\n",
        "    for i in range(len(Xb)): # Iterate through batch\n",
        "        if count >= num_samples_to_evaluate:\n",
        "            break\n",
        "\n",
        "        x_eeg = Xb[i:i+1] # Single EEG sample\n",
        "\n",
        "        # 1. Predict emotion from EEG\n",
        "        with torch.no_grad():\n",
        "            logits = model(x_eeg)\n",
        "            pred_class = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "        # 2. Get corresponding text prompt\n",
        "        prompt_text = emotion_prompts[pred_class]\n",
        "\n",
        "        # 3. Generate image using Stable Diffusion with the text prompt\n",
        "        generated_image = pipe(\n",
        "            prompt=prompt_text,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=25, # Fewer steps for speed\n",
        "            guidance_scale=7.5,\n",
        "            height=512,\n",
        "            width=512\n",
        "        ).images[0]\n",
        "\n",
        "        # 4. Encode generated image into CLIP embedding and normalize\n",
        "        image_inputs = clip_processor(images=generated_image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            # Use the dedicated method to get projected image features\n",
        "            image_embedding = clip_model.get_image_features(**image_inputs)\n",
        "            image_embedding_norm = F.normalize(image_embedding, p=2, dim=-1)\n",
        "\n",
        "        # 5. Encode the text prompt into CLIP embedding and normalize\n",
        "        text_embedding = get_clip_text_embedding(prompt_text)\n",
        "        text_embedding_norm = F.normalize(text_embedding, p=2, dim=-1)\n",
        "\n",
        "        # Verify dimensions before calculation\n",
        "        if batch_idx == 0 and i == 0: # Print shapes only for the first sample\n",
        "            print(f\"DEBUG: Image embedding shape: {image_embedding_norm.shape}\")\n",
        "            print(f\"DEBUG: Text embedding shape: {text_embedding_norm.shape}\")\n",
        "\n",
        "        # 6. Calculate cosine similarity\n",
        "        # Cosine similarity is the dot product of L2-normalized vectors\n",
        "        similarity = torch.sum(image_embedding_norm * text_embedding_norm).item()\n",
        "        consistency_scores.append(similarity)\n",
        "\n",
        "        image_embeddings_list.append(image_embedding.squeeze(0).cpu().numpy())\n",
        "        text_embeddings_list.append(text_embedding.squeeze(0).cpu().numpy())\n",
        "\n",
        "        count += 1\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "image_embeddings = np.array(image_embeddings_list)\n",
        "text_embeddings = np.array(text_embeddings_list)\n",
        "consistency_scores = np.array(consistency_scores)\n",
        "\n",
        "print(f\"Collected {len(image_embeddings)} image-text embedding pairs and {len(consistency_scores)} consistency scores.\")\n",
        "print(\"Image embeddings shape:\", image_embeddings.shape)\n",
        "print(\"Text embeddings shape:\", text_embeddings.shape)\n",
        "print(\"Consistency scores shape:\", consistency_scores.shape)\n",
        "\n",
        "# Plotting the distribution of consistency scores\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.histplot(consistency_scores, bins=20, kde=True)\n",
        "plt.title('Distribution of Image-Emotion Consistency Scores (Cosine Similarity)')\n",
        "plt.xlabel('Cosine Similarity Score (Image Embedding vs. Text Embedding)')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(axis='y', alpha=0.75)\n",
        "plt.show()\n",
        "\n",
        "# Summarize findings\n",
        "mean_score = np.mean(consistency_scores)\n",
        "median_score = np.median(consistency_scores)\n",
        "std_dev_score = np.std(consistency_scores)\n",
        "\n",
        "print(f\"\\nSummary of Image-Emotion Consistency Scores:\")\n",
        "print(f\"Mean Cosine Similarity: {mean_score:.4f}\")\n",
        "print(f\"Median Cosine Similarity: {median_score:.4f}\")\n",
        "print(f\"Standard Deviation: {std_dev_score:.4f}\")\n",
        "print(f\"Min Score: {np.min(consistency_scores):.4f}\")\n",
        "print(f\"Max Score: {np.max(consistency_scores):.4f}\")\n",
        "\n",
        "# Discussion of findings based on the distribution\n",
        "if mean_score > 0.7:\n",
        "    print(\"\\nThe high mean cosine similarity suggests a strong alignment between the generated images and their corresponding emotion text prompts. The Stable Diffusion model, guided by the EEG-predicted emotions, is likely producing images that effectively capture the semantic meaning of the emotion prompts.\")\n",
        "elif mean_score > 0.5:\n",
        "    print(\"\\nThe moderate mean cosine similarity indicates a reasonable alignment between the generated images and their corresponding emotion text prompts. There's a good general correlation, but there might be room for improvement in fine-tuning prompts or the adapter for stronger consistency.\")\n",
        "else:\n",
        "    print(\"\\nThe low mean cosine similarity suggests a weak alignment between the generated images and their corresponding emotion text prompts. This could indicate that either the EEG-to-CLIP adapter needs further training, the emotion prompts are not sufficiently descriptive, or the Stable Diffusion model struggles to interpret the nuances of these emotion prompts effectively.\")\n",
        "\n",
        "print(f\"\\nDetailed analysis of the histogram shows the distribution of scores. A distribution clustered towards higher values (closer to 1) indicates better consistency, while a spread towards lower values (closer to 0 or negative) suggests less alignment. The shape of the KDE (Kernel Density Estimate) also provides insight into the overall quality and reliability of the image generation process based on emotion predictions.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43dc701f"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Summarize the findings from the consistency score distribution, discussing the extent to which generated images align with predicted emotions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28781ce5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Q&A\n",
        "The generated images show a weak alignment with the predicted emotions, as indicated by a mean cosine similarity score of 0.2352. The distribution of consistency scores is low, ranging from 0.1918 to 0.2814.\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   **Initial Dimension Mismatch:** The initial attempt to calculate consistency scores using `CLIPVisionModel` and `CLIPTextModel` from `openai/clip-vit-large-patch14` failed due to a `RuntimeError` caused by a tensor size mismatch (1024 vs. 768 dimensions).\n",
        "*   **Persistent Mismatch:** An subsequent attempt with `openai/clip-vit-base-patch32` also resulted in dimension inconsistency (768 for image embeddings and 512 for text embeddings), demonstrating that using separate `CLIPVisionModel` and `CLIPTextModel` components does not guarantee consistent embedding dimensions across modalities, even with the same base model.\n",
        "*   **Resolution of Dimension Inconsistency:** The issue was successfully resolved by loading the comprehensive `CLIPModel` (`openai/clip-vit-large-patch14`) and utilizing its dedicated `get_image_features()` and `get_text_features()` methods. This ensured both image and text embeddings were consistently 768-dimensional.\n",
        "*   **Consistency Score Summary:** For 50 evaluated samples, the image-emotion consistency scores (cosine similarity) were summarized as follows:\n",
        "    *   Mean Cosine Similarity: 0.2352\n",
        "    *   Median Cosine Similarity: 0.2318\n",
        "    *   Standard Deviation: 0.0191\n",
        "    *   Minimum Score: 0.1918\n",
        "    *   Maximum Score: 0.2814\n",
        "*   **Weak Alignment:** The mean cosine similarity of 0.2352 indicates a weak alignment between the generated images and their corresponding emotion text prompts.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Further investigation is needed to determine whether the weak alignment stems from the EEG-to-CLIP adapter's effectiveness, the descriptiveness of the emotion prompts, or the Stable Diffusion model's interpretation of these emotion prompts.\n",
        "*   Consider strategies to improve the consistency, such as fine-tuning the EEG-to-CLIP adapter, refining the emotion prompts used for image generation, or exploring alternative image generation models/techniques.\n"
      ]
    }
  ]
}